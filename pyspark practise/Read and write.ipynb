{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf6c020-c72a-41db-a5ee-137ce770a224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_poc = spark.sql(\"select * from uc_poc.poc_stg.customers_stg\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70aa7afb-93a5-4236-8a72-044159aeb06d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_poc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b93259b6-6237-42a4-a801-aff4c24d4d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89c19a08-7296-47f3-b269-6003ea49b7fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, FloatType\n",
    "\n",
    "data = {\n",
    "    \"id\": 1,\n",
    "    \"name\": \"Alice\",\n",
    "    \"orders\": [\n",
    "        {\n",
    "            \"order_id\": \"100\",\n",
    "            \"items\": [\n",
    "                {\"item_id\": \"a1\", \"price\": 10},\n",
    "                {\"item_id\": \"a2\", \"price\": 15}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"order_id\": \"101\",\n",
    "            \"items\": [\n",
    "                {\"item_id\": \"b1\", \"price\": 7}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"orders\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"order_id\", StringType(), True),\n",
    "            StructField(\"items\", ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"item_id\", StringType(), True),\n",
    "                    StructField(\"price\", FloatType(), True)\n",
    "                ])\n",
    "            ), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame([data], schema=schema)\n",
    "df_orders = df.select(\"id\", \"name\", explode(\"orders\").alias(\"order\"))\n",
    "df_items = df_orders.select(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    col(\"order.order_id\").alias(\"order_id\"),\n",
    "    explode(\"order.items\").alias(\"item\")\n",
    ").select(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"order_id\",\n",
    "    col(\"item.item_id\").alias(\"item_id\"),\n",
    "    col(\"item.price\").alias(\"price\")\n",
    ")\n",
    "\n",
    "display(df_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5044c3bf-4472-4827-baed-17cfc90a777f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6e7f5e-ea39-4be0-bd01-fa95ef345a43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT a.Country || '-' || b.Country AS Country_Pair\n",
    "FROM VALUES ('Ind'), ('SL'), ('Pak'), ('Ban') AS a(Country),\n",
    "     VALUES ('Ind'), ('SL'), ('Pak'), ('Ban') AS b(Country)\n",
    "WHERE a.Country < b.Country\n",
    "ORDER BY a.Country, b.Country;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72069b4d-59af-463b-a2f6-022df4cd3029",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW Country AS\n",
    "SELECT 'Ind' AS Country UNION ALL\n",
    "SELECT 'SL' UNION ALL\n",
    "SELECT 'Pak' UNION ALL\n",
    "SELECT 'Ban';\n",
    "\n",
    "SELECT concat(a.Country,\"-\",  b.Country) AS Country_Pair\n",
    "FROM Country a, Country b\n",
    "WHERE a.Country < b.Country;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2489290-ce74-4dc3-9d1e-8e2e95042567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, trim,col\n",
    "\n",
    "data = [\n",
    "    (1, \"Gaurav\",\"Pune,Bangalore,Hyderabad\"),\n",
    "    (2, \"Risabh\",\"Mumbai,Bangalore,Pune\")\n",
    "]\n",
    "\n",
    "columns = [\"EmpId\", \"Name\", \"Locations\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df_split  = df.withColumn(\"listlocation\",split(col(\"Locations\"),\",\"))\n",
    "display(df_split)\n",
    " \n",
    " \n",
    "df_output = df_split.select(\"empid\",\"Name\",explode(col(\"listlocation\")).alias(\"locations\")) \n",
    "\n",
    "#df_split = df.withColumn(\"Location\", explode(split(\"Locations\", \",\")))\n",
    "#df_final = df_split.withColumn(\"Location\", trim(df_split[\"Location\"])).select(\"EmpId\", \"Name\", \"Location\")\n",
    "\n",
    "display(df_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5323175-4357-4cf4-9883-4506c480d139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "data = [(1,), (2,), (4,), (5,), (-1,), (-2,), (-3,), (-4,)]\n",
    "\n",
    "columns = [\"colname\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df_positive = df.filter(col(\"colname\") > 0).withColumnRenamed(\"colname\", \"positive_column\")\n",
    "df_negative = df.filter(col(\"colname\") < 0).withColumnRenamed(\"colname\", \"negative_col\")\n",
    "\n",
    "df_output = df_negative.withColumn(\"row_num\", row_number().over(Window.orderBy(\"negative_col\"))) \\\n",
    "    .join(df_positive.withColumn(\"row_num\", row_number().over(Window.orderBy(\"positive_column\"))), \"row_num\") \\\n",
    "    .select(\"negative_col\", \"positive_column\")\n",
    "\n",
    "display(df_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8abf9f-96ed-4aca-bb99-ef7eadb79c86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS sales_data (\n",
    "product_name VARCHAR( 100), category VARCHAR(50), sales DECIMAL (10, 2)\n",
    ");\n",
    "INSERT\n",
    "INTO sales_data(product_name, category, sales)\n",
    "VALUES\n",
    "('Product A','Category 1',1000.00),\n",
    "('Product B', 'Category 1', 2000.00),\n",
    "('Product C', 'Category 2', 1500.00),\n",
    "('Product D', 'Category 2', 2500.00)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9b91d7-1e2d-443c-afa1-96a8f3767005",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "total sales"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *, cast(sum(sales) over(partition by category) as string) as total_sales from sales_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c661d8c-c87d-41e8-b83f-a6153b40c319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with salesrank as\n",
    "(select product_name,category,sales ,dense_rank() over (partition by category order by sales desc) as rank  from sales_data)\n",
    "\n",
    "select * from salesrank where rank=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abec08f8-9afa-437e-b243-7a68c4e86fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "l1 = [1, 1, 2, 2, 3, 4, 4, 4, 5, 6]\n",
    "\n",
    "result = dict(Counter(l1))\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c41e5f8c-ee1d-41ca-9420-26702f9f8ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "data = [(1,), (2,), (4,), (5,), (-1,), (-2,), (-3,), (-4,)]\n",
    "\n",
    "columns = [\"colname\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "display(df)\n",
    "\n",
    "df_positive = df.filter(col(\"colname\") > 0).withColumnRenamed(\"colname\", \"positive_column\")\n",
    "df_negative = df.filter(col(\"colname\") < 0).withColumnRenamed(\"colname\", \"negative_column\")\n",
    "\n",
    "df_output = df_positive.withColumn(\"id\",row_number().over(Window.orderBy(\"positive_column\"))).join(df_negative.withColumn(\"id\",row_number().over(Window.orderBy(\"negative_column\"))),\"id\").select(\"negative_column\",\"positive_column\")\n",
    "display(df_output)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8866507798509502,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Read and write",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
